{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of environments and dependencies from source. Make sure you have installed pip and your python version is greater that 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %python -m venv stable-diffusion-controlnet\n",
    "# %source stable-diffusion-controlnet/bin/activate\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer device for openvino model will be used, in this example we set GPU.1(Arc770), if you want to use CPU, please set it as \"CPU\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_NAME=\"GPU.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two kinds of compile config for data type, it's noticed that vae_decoder part of sdxl model is not supported under f16 precison by using intel gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILE_CONFIG_FP32 = {'INFERENCE_PRECISION_HINT': 'f32'}\n",
    "COMPILE_CONFIG_FP16 = {'INFERENCE_PRECISION_HINT': 'f16'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All compoents of sdxl model save path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_OV_PATH = Path(\"./models_ov/unet/openvino_model.xml\")\n",
    "CONTROLNET_OV_PATH = Path(\"./models_ov/controlnet/openvino_model.xml\")\n",
    "TEXT_ENCODER_OV_PATH = Path(\"./models_ov/encoder/openvino_model.xml\")\n",
    "TEXT_ENCODER_2_OV_PATH = Path(\"./models_ov/encoder_2/openvino_model.xml\")\n",
    "TOKENIZER_OV_PATH = Path(\"./models_ov/tokenizer\")\n",
    "TOKENIZER_2_OV_PATH = Path(\"./models_ov/tokenizer_2\")\n",
    "SCHEDULER_OV_PATH = Path(\"./models_ov/scheduler\")\n",
    "VAE_DECODER_OV_PATH = Path(\"./models_ov/vae_decoder/openvino_model.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all functionalities of openvino, we need a core. And we define a help function for removing cached model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build StableDiffusionXLControlNetPipeline, pytorch backend. It is noticed that torch_dtype=torch.float32 is nessesary for cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though original pytorch model can be directly download by from_pretrained() function, it is still recommended to manully download pre-trained model to **./models_torch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UNET_OV_PATH.exists() and CONTROLNET_OV_PATH.exists() and TEXT_ENCODER_OV_PATH.exists() and TEXT_ENCODER_2_OV_PATH.exists() and VAE_DECODER_OV_PATH.exists():\n",
    "    print(\"Loading OpenVINO models\")\n",
    "elif (Path(\"./models_torch/mistoLine\")).exists() and (Path(\"./models_torch/stable-diffusion-xl-base-1.0\")).exists():\n",
    "    controlnet = ControlNetModel.from_pretrained(\"./models_torch/mistoLine\",torch_dtype=torch.float32,variant=\"fp16\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\", torch_dtype=torch.float32)\n",
    "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\"./models_torch/stable-diffusion-xl-base-1.0\", vae=vae, controlnet=controlnet)\n",
    "else:\n",
    "    controlnet = ControlNetModel.from_pretrained(\"TheMistoAI/MistoLine\",torch_dtype=torch.float32,variant=\"fp16\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\", torch_dtype=torch.float32)\n",
    "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", vae=vae, controlnet=controlnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After built StableDiffusionXLControlNetPipeline, we save configs of tokenzier, tokenzier_2 and scheduler, which will not be converted from pytorch model, rahter, we will use it from configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config save\n",
    "if not TOKENIZER_OV_PATH.exists():\n",
    "    pipe.tokenizer.save_pretrained(TOKENIZER_OV_PATH)\n",
    "\n",
    "if not TOKENIZER_2_OV_PATH.exists():\n",
    "    pipe.tokenizer_2.save_pretrained(TOKENIZER_2_OV_PATH)\n",
    "\n",
    "if not SCHEDULER_OV_PATH.exists():\n",
    "    pipe.scheduler.save_config(SCHEDULER_OV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a Wrapper class to wrap the original controlnet torch forward function. And give example input and set input_info for dynamic model convert.Then we can convert controlnet to Openvino IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#controlnet\n",
    "class ControlnetWrapper(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            controlnet,\n",
    "            sample_dtype=torch.float32,\n",
    "            timestep_dtype=torch.int64,\n",
    "            encoder_hidden_states_dtype=torch.float32,\n",
    "            controlnet_cond_dtype=torch.float32,\n",
    "            text_embeds_dtype=torch.float32,\n",
    "            time_ids_dtype=torch.int64,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.controlnet = controlnet\n",
    "        self.sample_dtype = sample_dtype\n",
    "        self.timestep_dtype = timestep_dtype\n",
    "        self.encoder_hidden_states_dtype = encoder_hidden_states_dtype\n",
    "        self.controlnet_cond_dtype = controlnet_cond_dtype\n",
    "        self.text_embeds_dtype = text_embeds_dtype\n",
    "        self.time_ids_dtype = time_ids_dtype\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            sample: torch.Tensor, \n",
    "            timestep: torch.Tensor, \n",
    "            encoder_hidden_states: torch.Tensor, \n",
    "            controlnet_cond: torch.Tensor, \n",
    "            text_embeds: torch.Tensor, \n",
    "            time_ids: torch.Tensor, \n",
    "        ):\n",
    "            sample.to(self.sample_dtype)\n",
    "            timestep.to(self.timestep_dtype)\n",
    "            encoder_hidden_states.to(self.encoder_hidden_states_dtype)\n",
    "            controlnet_cond.to(self.controlnet_cond_dtype)\n",
    "            text_embeds.to(self.text_embeds_dtype)\n",
    "            time_ids.to(self.time_ids_dtype)\n",
    "            added_cond_kwargs = {\"text_embeds\": text_embeds, \"time_ids\": time_ids}\n",
    "            return self.controlnet(\n",
    "            sample=sample, \n",
    "            timestep=timestep, \n",
    "            encoder_hidden_states=encoder_hidden_states, \n",
    "            controlnet_cond=controlnet_cond, \n",
    "            added_cond_kwargs=added_cond_kwargs,\n",
    "            return_dict=False,\n",
    "            )\n",
    "    \n",
    "inputs = {\n",
    "    \"sample\": torch.randn((2, 4, 128, 128)),\n",
    "    \"timestep\": torch.tensor(1),\n",
    "    \"encoder_hidden_states\": torch.randn((2, 77, 2048)),\n",
    "    \"controlnet_cond\": torch.randn((2, 3, 1024, 1024)),\n",
    "    \"text_embeds\": torch.randn((2, 1280)),\n",
    "    \"time_ids\": torch.randn((2, 6)),\n",
    "}\n",
    "\n",
    "input_info = []\n",
    "for name, inp in inputs.items():\n",
    "    shape = ov.PartialShape(inp.shape)\n",
    "    # element_type = dtype_mapping[input_tensor.dtype]\n",
    "    if len(shape) == 4:\n",
    "        shape[0] = -1\n",
    "        shape[2] = -1\n",
    "        shape[3] = -1\n",
    "    elif len(shape) == 3:\n",
    "        shape[0] = -1\n",
    "        shape[1] = -1\n",
    "    elif len(shape) == 2:\n",
    "        shape[0] = -1\n",
    "        shape[1] = -1\n",
    "    input_info.append((shape))\n",
    "\n",
    "\n",
    "if not CONTROLNET_OV_PATH.exists():\n",
    "\n",
    "    controlnet=ControlnetWrapper(controlnet)\n",
    "    controlnet.eval()\n",
    "    with torch.no_grad():\n",
    "        down_block_res_samples, mid_block_res_sample = controlnet(**inputs)\n",
    "        ov_model = ov.convert_model(controlnet, example_input=inputs, input=input_info)\n",
    "        ov.save_model(ov_model, CONTROLNET_OV_PATH)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "    print(\"ControlNet successfully converted to IR\")\n",
    "    del controlnet\n",
    "else:\n",
    "    print(f\"ControlNet will be loaded from {CONTROLNET_OV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then same for Unet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet\n",
    "dtype_mapping = {\n",
    "    torch.float32: ov.Type.f32,\n",
    "    torch.float64: ov.Type.f64,\n",
    "    torch.int32: ov.Type.i32,\n",
    "    torch.int64: ov.Type.i64,\n",
    "}\n",
    "\n",
    "\n",
    "class UnetWrapper(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet,\n",
    "        sample_dtype=torch.float32,\n",
    "        timestep_dtype=torch.int64,\n",
    "        encoder_hidden_states_dtype=torch.float32,\n",
    "        text_embeds_dtype=torch.float32,\n",
    "        time_ids_dtype=torch.int64,\n",
    "        down_block_additional_residuals_dtype=torch.float32,\n",
    "        mid_block_additional_residual_dtype=torch.float32,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.sample_dtype = sample_dtype\n",
    "        self.timestep_dtype = timestep_dtype\n",
    "        self.encoder_hidden_states_dtype = encoder_hidden_states_dtype\n",
    "        self.text_embeds_dtype = text_embeds_dtype\n",
    "        self.time_ids_dtype = time_ids_dtype\n",
    "        self.down_block_additional_residuals_dtype = down_block_additional_residuals_dtype\n",
    "        self.mid_block_additional_residual_dtype = mid_block_additional_residual_dtype\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.Tensor,\n",
    "        timestep: torch.Tensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        text_embeds: torch.Tensor, \n",
    "        time_ids: torch.Tensor, \n",
    "        down_block_additional_residuals: Tuple[torch.Tensor],\n",
    "        mid_block_additional_residual: torch.Tensor,\n",
    "    ):\n",
    "        sample.to(self.sample_dtype)\n",
    "        timestep.to(self.timestep_dtype)\n",
    "        encoder_hidden_states.to(self.encoder_hidden_states_dtype)\n",
    "        text_embeds.to(self.text_embeds_dtype)\n",
    "        time_ids.to(self.time_ids_dtype)\n",
    "        added_cond_kwargs = {\"text_embeds\": text_embeds, \"time_ids\": time_ids}\n",
    "        down_block_additional_residuals = [res.to(self.down_block_additional_residuals_dtype) for res in down_block_additional_residuals]\n",
    "        mid_block_additional_residual.to(self.mid_block_additional_residual_dtype)\n",
    "        return self.unet(\n",
    "            sample,\n",
    "            timestep,\n",
    "            encoder_hidden_states,\n",
    "            down_block_additional_residuals=down_block_additional_residuals,\n",
    "            mid_block_additional_residual=mid_block_additional_residual,\n",
    "            added_cond_kwargs=added_cond_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def flattenize_inputs(inputs):\n",
    "    flatten_inputs = []\n",
    "    for input_data in inputs:\n",
    "        if input_data is None:\n",
    "            continue\n",
    "        if isinstance(input_data, (list, tuple)):\n",
    "            flatten_inputs.extend(flattenize_inputs(input_data))\n",
    "        else:\n",
    "            flatten_inputs.append(input_data)\n",
    "    return flatten_inputs\n",
    "\n",
    "\n",
    "if not UNET_OV_PATH.exists():\n",
    "    inputs.pop(\"controlnet_cond\", None)\n",
    "    \n",
    "    inputs[\"down_block_additional_residuals\"] = down_block_res_samples\n",
    "    inputs[\"mid_block_additional_residual\"] = mid_block_res_sample\n",
    "\n",
    "    unet = UnetWrapper(pipe.unet)\n",
    "    unet.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(unet, example_input=inputs)\n",
    "  \n",
    "    flatten_inputs = flattenize_inputs(inputs.values())\n",
    "    a = 1\n",
    "\n",
    "    for input_data, input_tensor in zip(flatten_inputs, ov_model.inputs):\n",
    "        r_name = input_tensor.get_node().get_friendly_name()\n",
    "        r_shape = ov.PartialShape(input_data.shape)\n",
    "        print(\"============\")\n",
    "        print(r_name, r_shape)\n",
    "        \n",
    "        if len(r_shape) == 4:\n",
    "            r_shape[0] = -1\n",
    "            r_shape[2] = -1\n",
    "            r_shape[3] = -1\n",
    "        elif len(r_shape) == 3:\n",
    "            r_shape[0] = -1\n",
    "            r_shape[1] = -1\n",
    "        elif len(r_shape) == 2:\n",
    "            r_shape[0] = -1\n",
    "            r_shape[1] = -1\n",
    "        tn = \"down_block_additional_residual_\"\n",
    "        if r_name not in [\"sample\", \"timestep\", \"encoder_hidden_states\", \"mid_block_additional_residual\", \"text_embeds\", \"time_ids\"] and len(r_shape)==4:\n",
    "            n_name = tn + str(a)\n",
    "            if a == 17:\n",
    "                n_name = \"down_block_additional_residual\"\n",
    "            input_tensor.get_node().set_friendly_name(n_name)\n",
    "            a = a + 2\n",
    "        print(input_tensor.get_node().get_friendly_name(), r_shape)\n",
    "        print(\"============\")\n",
    "        input_tensor.get_node().set_partial_shape(r_shape)\n",
    "        input_tensor.get_node().set_element_type(dtype_mapping[input_data.dtype])\n",
    "\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "    ov.save_model(ov_model, UNET_OV_PATH)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    del unet\n",
    "    del pipe.unet\n",
    "    print(\"Unet successfully converted to IR\")\n",
    "else:\n",
    "    print(f\"Unet will be loaded from {UNET_OV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then same for encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoder(text_encoder: torch.nn.Module, ir_path: Path):\n",
    "    \"\"\"\n",
    "    Convert Text Encoder model to OpenVINO IR.\n",
    "    Function accepts text encoder model, prepares example inputs for conversion, and convert it to OpenVINO Model\n",
    "    Parameters:\n",
    "        text_encoder (torch.nn.Module): text_encoder model\n",
    "        ir_path (Path): File for storing model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not ir_path.exists():\n",
    "        input_ids = torch.ones((1, 77), dtype=torch.int64)\n",
    "        # switch model to inference mode\n",
    "        text_encoder.eval()\n",
    "        text_encoder.config.output_hidden_states = True\n",
    "        text_encoder.config.return_dict = False\n",
    "        # disable gradients calculation for reducing memory consumption\n",
    "        with torch.no_grad():\n",
    "            ov_model = ov.convert_model(\n",
    "                text_encoder,  # model instance\n",
    "                example_input=input_ids,  # inputs for model tracing\n",
    "                input=([1, 77],),\n",
    "            )\n",
    "            ov.save_model(ov_model, ir_path)\n",
    "            del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        print(\"Text Encoder successfully converted to IR\")\n",
    "\n",
    "\n",
    "if not TEXT_ENCODER_OV_PATH.exists():\n",
    "    convert_encoder(pipe.text_encoder, TEXT_ENCODER_OV_PATH)\n",
    "    del pipe.text_encoder\n",
    "else:\n",
    "    print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV_PATH}\")\n",
    "\n",
    "if not TEXT_ENCODER_2_OV_PATH.exists():\n",
    "    convert_encoder(pipe.text_encoder_2, TEXT_ENCODER_2_OV_PATH)\n",
    "    del pipe.text_encoder_2\n",
    "else:\n",
    "    print(f\"Text encoder_2 will be loaded from {TEXT_ENCODER_2_OV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then same for vae_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vae_decoder(vae: torch.nn.Module, ir_path: Path):\n",
    "    \"\"\"\n",
    "    Convert VAE model to IR format.\n",
    "    Function accepts pipeline, creates wrapper class for export only necessary for inference part,\n",
    "    prepares example inputs for convert,\n",
    "    Parameters:\n",
    "        vae (torch.nn.Module): VAE model\n",
    "        ir_path (Path): File for storing model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    class VAEDecoderWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            return self.vae.decode(latents)\n",
    "\n",
    "    if not ir_path.exists():\n",
    "        vae_decoder = VAEDecoderWrapper(vae)\n",
    "        latent_sample = torch.randn((1, 4, 128, 128))\n",
    "\n",
    "        vae_decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            ov_model = ov.convert_model(\n",
    "                vae_decoder,\n",
    "                example_input=latent_sample,\n",
    "                input=[\n",
    "                    (-1, 4, -1, -1),\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            ov.save_model(ov_model, ir_path)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        print(\"VAE decoder successfully converted to IR\")\n",
    "\n",
    "\n",
    "if not VAE_DECODER_OV_PATH.exists():\n",
    "    convert_vae_decoder(pipe.vae, VAE_DECODER_OV_PATH)\n",
    "else:\n",
    "    print(f\"VAE decoder will be loaded from {VAE_DECODER_OV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we convert and save all nessesary part of stable diffusion xl and controlnet dynamic openvino models. Consider of GPU memory decrease and load model time decrease. We need to reshape dynamic shape model to static shape model and save model_cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_STATIC = True\n",
    "STATIC_SHAPE = [1024,1024]\n",
    "\n",
    "UNET_STATIC_OV_PATH = Path(\"./models_ov_static/unet/openvino_model.xml\")\n",
    "CONTROLNET_STATIC_OV_PATH = Path(\"./models_ov_static/controlnet/openvino_model.xml\")\n",
    "TEXT_ENCODER_STATIC_OV_PATH = Path(\"./models_ov_static/encoder/openvino_model.xml\")\n",
    "TEXT_ENCODER_STATIC_2_OV_PATH = Path(\"./models_ov_static/encoder_2/openvino_model.xml\")\n",
    "VAE_DECODER_STATIC_OV_PATH = Path(\"./models_ov_static/vae_decoder/openvino_model.xml\")\n",
    "\n",
    "\n",
    "def reshape(\n",
    "        batch_size: int = -1,\n",
    "        height: int = -1,\n",
    "        width: int = -1,\n",
    "        num_images_per_prompt: int = -1,\n",
    "        tokenizer_max_length: int = -1,\n",
    "):\n",
    "    if not CONTROLNET_STATIC_OV_PATH.exists():\n",
    "        controlnet = core.read_model(CONTROLNET_OV_PATH)\n",
    "        def reshape_controlnet(\n",
    "                model: ov.runtime.Model,\n",
    "                batch_size: int = -1,\n",
    "                height: int = -1,\n",
    "                width: int = -1,\n",
    "                num_images_per_prompt: int = -1,\n",
    "                tokenizer_max_length: int = -1,\n",
    "            ):\n",
    "                if batch_size == -1 or num_images_per_prompt == -1:\n",
    "                    batch_size = -1\n",
    "                else:\n",
    "                    batch_size *= num_images_per_prompt\n",
    "                    # The factor of 2 comes from the guidance scale > 1\n",
    "                    if \"timestep_cond\" not in {inputs.get_node().get_friendly_name() for inputs in model.inputs}:\n",
    "                        batch_size *= 2\n",
    "\n",
    "                height_ = height // 8 if height > 0 else height\n",
    "                width_ = width // 8 if width > 0 else width\n",
    "                shapes = {}\n",
    "                for inputs in model.inputs:\n",
    "                    shapes[inputs] = inputs.get_partial_shape()\n",
    "                    if inputs.get_node().get_friendly_name() == \"timestep\":\n",
    "                        shapes[inputs] = shapes[inputs]\n",
    "                    elif inputs.get_node().get_friendly_name() == \"sample\":\n",
    "                        shapes[inputs] = [2, 4, height_, width_]\n",
    "                    elif inputs.get_node().get_friendly_name() == \"controlnet_cond\":\n",
    "                        shapes[inputs][0] = batch_size\n",
    "                        shapes[inputs][2] = height \n",
    "                        shapes[inputs][3] = width  \n",
    "                    elif inputs.get_node().get_friendly_name() == \"time_ids\":\n",
    "                        shapes[inputs] = [batch_size, 6]\n",
    "                    elif inputs.get_node().get_friendly_name() == \"text_embeds\":\n",
    "                        shapes[inputs] = [batch_size, 1280]\n",
    "                    elif inputs.get_node().get_friendly_name() == \"encoder_hidden_states\":\n",
    "                        shapes[inputs][0] = batch_size\n",
    "                        shapes[inputs][1] = tokenizer_max_length\n",
    "                model.reshape(shapes)\n",
    "                model.validate_nodes_and_infer_types()\n",
    "                \n",
    "        reshape_controlnet(controlnet, batch_size, height, width, num_images_per_prompt, tokenizer_max_length)\n",
    "        ov.save_model(controlnet, CONTROLNET_STATIC_OV_PATH)\n",
    "\n",
    "    if not UNET_STATIC_OV_PATH.exists():\n",
    "        unet = core.read_model(UNET_OV_PATH)\n",
    "        def reshape_unet_controlnet(\n",
    "            model: ov.runtime.Model,\n",
    "            batch_size: int = -1,\n",
    "            height: int = -1,\n",
    "            width: int = -1,\n",
    "            num_images_per_prompt: int = -1,\n",
    "            tokenizer_max_length: int = -1,\n",
    "        ):\n",
    "            if batch_size == -1 or num_images_per_prompt == -1:\n",
    "                batch_size = -1\n",
    "            else:\n",
    "                batch_size *= num_images_per_prompt\n",
    "                # The factor of 2 comes from the guidance scale > 1\n",
    "                if \"timestep_cond\" not in {inputs.get_node().get_friendly_name() for inputs in model.inputs}:\n",
    "                    batch_size *= 2\n",
    "\n",
    "            height = height // 8 if height > 0 else height\n",
    "            width = width // 8 if width > 0 else width\n",
    "            shapes = {}\n",
    "            for inputs in model.inputs:\n",
    "                shapes[inputs] = inputs.get_partial_shape()\n",
    "                if inputs.get_node().get_friendly_name() == \"timestep\":\n",
    "                    shapes[inputs] = shapes[inputs]\n",
    "                elif inputs.get_node().get_friendly_name() == \"sample\":\n",
    "                    shapes[inputs] = [2, 4, height, width]\n",
    "                elif inputs.get_node().get_friendly_name() == \"text_embeds\":\n",
    "                    shapes[inputs] = [batch_size, 1280]\n",
    "                elif inputs.get_node().get_friendly_name() == \"time_ids\":\n",
    "                    shapes[inputs] = [batch_size, 6]\n",
    "                elif inputs.get_node().get_friendly_name() == \"encoder_hidden_states\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][1] = tokenizer_max_length\n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_1\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height \n",
    "                    shapes[inputs][3] = width    \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_3\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height  \n",
    "                    shapes[inputs][3] = width      \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_5\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height   \n",
    "                    shapes[inputs][3] = width     \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_7\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 2 \n",
    "                    shapes[inputs][3] = width // 2  \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_9\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 2 \n",
    "                    shapes[inputs][3] = width // 2      \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_11\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 2 \n",
    "                    shapes[inputs][3] = width // 2    \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_13\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 4 \n",
    "                    shapes[inputs][3] = width // 4    \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual_15\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 4 \n",
    "                    shapes[inputs][3] = width // 4    \n",
    "                elif inputs.get_node().get_friendly_name() == \"down_block_additional_residual\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 4 \n",
    "                    shapes[inputs][3] = width // 4   \n",
    "                elif inputs.get_node().get_friendly_name() == \"mid_block_additional_residual\":\n",
    "                    shapes[inputs][0] = batch_size\n",
    "                    shapes[inputs][2] = height // 4 \n",
    "                    shapes[inputs][3] = width // 4   \n",
    "\n",
    "            model.reshape(shapes)\n",
    "            model.validate_nodes_and_infer_types()\n",
    "\n",
    "        reshape_unet_controlnet(unet, batch_size, height, width, num_images_per_prompt, tokenizer_max_length)\n",
    "        ov.save_model(unet, UNET_STATIC_OV_PATH)\n",
    "\n",
    "    if not TEXT_ENCODER_STATIC_OV_PATH.exists() or  not TEXT_ENCODER_STATIC_2_OV_PATH.exists():\n",
    "        text_encoder = core.read_model(TEXT_ENCODER_OV_PATH)\n",
    "        text_encoder_2 = core.read_model(TEXT_ENCODER_2_OV_PATH)\n",
    "\n",
    "        def reshape_text_encoder(\n",
    "            model: ov.runtime.Model, batch_size: int = -1, tokenizer_max_length: int = -1\n",
    "        ):\n",
    "            if batch_size != -1:\n",
    "                shapes = {model.inputs[0]: [batch_size, tokenizer_max_length]}\n",
    "                model.reshape(shapes)\n",
    "                model.validate_nodes_and_infer_types()\n",
    "\n",
    "        reshape_text_encoder(text_encoder, 1, tokenizer_max_length)\n",
    "        reshape_text_encoder(text_encoder_2, 1, tokenizer_max_length)\n",
    "        ov.save_model(text_encoder, TEXT_ENCODER_STATIC_OV_PATH)\n",
    "        ov.save_model(text_encoder_2, TEXT_ENCODER_STATIC_2_OV_PATH)\n",
    "\n",
    "    if not VAE_DECODER_STATIC_OV_PATH.exists():\n",
    "        vae_decoder = core.read_model(VAE_DECODER_OV_PATH)\n",
    "        def reshape_vae_decoder(model: ov.runtime.Model, height: int = -1, width: int = -1):\n",
    "            height = height // 8 if height > -1 else height\n",
    "            width = width // 8 if width > -1 else width\n",
    "            latent_channels = 4\n",
    "            shapes = {model.inputs[0]: [1, latent_channels, height, width]}\n",
    "            model.reshape(shapes)\n",
    "            model.validate_nodes_and_infer_types()\n",
    "\n",
    "        reshape_vae_decoder(vae_decoder, height, width)\n",
    "        ov.save_model(vae_decoder, VAE_DECODER_STATIC_OV_PATH)\n",
    "\n",
    "reshape(\n",
    "    batch_size=1,\n",
    "    height=STATIC_SHAPE[0],\n",
    "    width=STATIC_SHAPE[1],\n",
    "    num_images_per_prompt=1,\n",
    "    tokenizer_max_length=77,\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have static openvino model in the local. Then we compile_model once for generate model_cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_cache_dir(path, config):\n",
    "    ov_config = config\n",
    "    ov_path = path\n",
    "    parent_dir = os.path.abspath(os.path.dirname(ov_path))\n",
    "    ov_config[\"CACHE_DIR\"] = os.path.join(parent_dir, \"model_cache\")\n",
    "    return ov_config\n",
    "\n",
    "controlnet = core.compile_model(CONTROLNET_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(CONTROLNET_STATIC_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "unet = core.compile_model(UNET_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(UNET_STATIC_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "text_encoder = core.compile_model(TEXT_ENCODER_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(TEXT_ENCODER_STATIC_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "text_encoder_2 = core.compile_model(TEXT_ENCODER_STATIC_2_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(TEXT_ENCODER_STATIC_2_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "vae_decoder = core.compile_model(VAE_DECODER_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(VAE_DECODER_STATIC_OV_PATH, COMPILE_CONFIG_FP32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build up a pipeline for openvino backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVStableDiffusionXLControlNetPipeline(StableDiffusionXLControlNetPipeline):\n",
    "    \"\"\"\n",
    "    OpenVINO inference pipeline for Stable Diffusion XL with ControlNet guidence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        scheduler,\n",
    "        unet: ov.Model,\n",
    "        controlnet: ov.Model,\n",
    "        tokenizer: CLIPTokenizer,   \n",
    "        tokenizer_2: CLIPTokenizer,\n",
    "        text_encoder: ov.Model,\n",
    "        text_encoder_2: ov.Model,\n",
    "        vae_decoder: ov.Model,\n",
    "        device: str = \"AUTO\",\n",
    "    ):\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text_encoder_2 = text_encoder_2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        self.controlnet = controlnet\n",
    "        self.unet = unet\n",
    "        self.vae_decoder = vae_decoder\n",
    "        self.scheduler = scheduler\n",
    "        self.vae_scale_factor = 8\n",
    "        self.vae_scaling_factor = 0.13025\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor, do_convert_rgb=True)\n",
    "        self.control_image_processor = VaeImageProcessor(\n",
    "            vae_scale_factor=self.vae_scale_factor,\n",
    "            do_convert_rgb=True,\n",
    "            do_normalize=False,\n",
    "        )\n",
    "        self._internal_dict = {}\n",
    "        self._progress_bar_config = {}\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        image: PIL.Image.Image = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 5.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        image_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n",
    "        guess_mode: bool = False,\n",
    "        control_guidance_start: Union[float, List[float]] = 0.0,\n",
    "        control_guidance_end: Union[float, List[float]] = 1.0,\n",
    "        original_size: Tuple[int, int] = None,\n",
    "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        target_size: Tuple[int, int] = None,\n",
    "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        # IP adapter\n",
    "        ip_adapter_scale=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        do_classifier_free_guidance = guidance_scale >= 1.0\n",
    "        # align format for control guidance\n",
    "        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n",
    "            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n",
    "        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n",
    "            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n",
    "        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n",
    "            control_guidance_start, control_guidance_end = (\n",
    "                [control_guidance_start],\n",
    "                [control_guidance_end],\n",
    "            )\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            negative_prompt_2,\n",
    "            lora_scale=None,\n",
    "            clip_skip=clip_skip,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare image\n",
    "        image = self.prepare_image(\n",
    "            image=image,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            batch_size=batch_size * num_images_per_prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "            guess_mode=guess_mode,\n",
    "        )\n",
    "        height, width = image.shape[-2:]\n",
    "\n",
    "        # 5. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 6. Prepare latent variables\n",
    "        num_channels_latents = 4\n",
    "        latents = self.prepare_latents(\n",
    "            int(batch_size) * int(num_images_per_prompt),\n",
    "            int(num_channels_latents),\n",
    "            int(height),\n",
    "            int(width),\n",
    "            dtype=torch.float32,\n",
    "            device=torch.device(\"cpu\"),\n",
    "            generator=generator,\n",
    "            latents=latents,\n",
    "        )\n",
    "\n",
    "        # 7. Prepare extra step kwargs.\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "        # 7.1 Create tensor stating which controlnets to keep\n",
    "        controlnet_keep = []\n",
    "        for i in range(len(timesteps)):\n",
    "            keeps = [1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e) for s, e in zip(control_guidance_start, control_guidance_end)]\n",
    "            controlnet_keep.append(keeps)\n",
    "\n",
    "        # 7.2 Prepare added time ids & embeddings\n",
    "        if isinstance(image, list):\n",
    "            original_size = original_size or image[0].shape[-2:]\n",
    "        else:\n",
    "            original_size = original_size or image.shape[-2:]\n",
    "        target_size = target_size or (height, width)\n",
    "\n",
    "        add_text_embeds = pooled_prompt_embeds\n",
    "        if self.text_encoder_2 is None:\n",
    "            text_encoder_projection_dim = pooled_prompt_embeds.shape[-1]\n",
    "        else:\n",
    "            text_encoder_projection_dim = 1280\n",
    "\n",
    "        add_time_ids = self._get_add_time_ids(\n",
    "            original_size,\n",
    "            crops_coords_top_left,\n",
    "            target_size,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "\n",
    "        if negative_original_size is not None and negative_target_size is not None:\n",
    "            negative_add_time_ids = self._get_add_time_ids(\n",
    "                negative_original_size,\n",
    "                negative_crops_coords_top_left,\n",
    "                negative_target_size,\n",
    "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "            )\n",
    "        else:\n",
    "            negative_add_time_ids = add_time_ids\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            prompt_embeds = np.concatenate([negative_prompt_embeds, prompt_embeds], axis=0)\n",
    "            add_text_embeds = np.concatenate([negative_pooled_prompt_embeds, add_text_embeds], axis=0)\n",
    "            add_time_ids = np.concatenate([negative_add_time_ids, add_time_ids], axis=0)\n",
    "\n",
    "        add_time_ids = np.tile(add_time_ids, (batch_size * num_images_per_prompt, 1))\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # controlnet(s) inference\n",
    "                control_model_input = {\n",
    "                    \"sample\": latent_model_input,\n",
    "                    \"timestep\": t,\n",
    "                    \"encoder_hidden_states\": prompt_embeds,\n",
    "                    \"controlnet_cond\": image,\n",
    "                    \"text_embeds\": add_text_embeds,\n",
    "                    \"time_ids\": add_time_ids,\n",
    "                }\n",
    "                \n",
    "                result = self.controlnet(\n",
    "                    control_model_input\n",
    "                    # [\n",
    "                    # latent_model_input, \n",
    "                    # t, \n",
    "                    # prompt_embeds, \n",
    "                    # image,\n",
    "                    # add_text_embeds, \n",
    "                    # add_time_ids,\n",
    "                    # ]\n",
    "                )\n",
    "                down_and_mid_block_samples = [sample * controlnet_conditioning_scale for _, sample in result.items()]\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet(\n",
    "                    [\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        prompt_embeds,\n",
    "                        add_text_embeds, \n",
    "                        add_time_ids, \n",
    "                        *down_and_mid_block_samples,\n",
    "                    ]\n",
    "                )[0]\n",
    "                \n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred[0], noise_pred[1]\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(\n",
    "                    torch.from_numpy(noise_pred),\n",
    "                    t,\n",
    "                    latents,\n",
    "                    **extra_step_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "                progress_bar.update()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            image = self.vae_decoder(latents / self.vae_scaling_factor)[0]\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            image = self.image_processor.postprocess(torch.from_numpy(image), output_type=output_type)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusionXLPipelineOutput(images=image)\n",
    "\n",
    "    def encode_prompt(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        prompt_2: Optional[str] = None,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        do_classifier_free_guidance: bool = True,\n",
    "        negative_prompt: Optional[str] = None,\n",
    "        negative_prompt_2: Optional[str] = None,\n",
    "        lora_scale: Optional[float] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Encodes the prompt into text encoder hidden states.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                prompt to be encoded\n",
    "            prompt_2 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n",
    "                used in both text-encoders\n",
    "            num_images_per_prompt (`int`):\n",
    "                number of images that should be generated per prompt\n",
    "            do_classifier_free_guidance (`bool`):\n",
    "                whether to use classifier free guidance or not\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
    "                less than `1`).\n",
    "            negative_prompt_2 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n",
    "                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n",
    "                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n",
    "            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n",
    "                input argument.\n",
    "            lora_scale (`float`, *optional*):\n",
    "                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n",
    "            clip_skip (`int`, *optional*):\n",
    "                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n",
    "                the output of the pre-final layer will be used for computing the prompt embeddings.\n",
    "        \"\"\"\n",
    "        prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "        # Define tokenizers and text encoders\n",
    "        tokenizers = [self.tokenizer, self.tokenizer_2] if self.tokenizer is not None else [self.tokenizer_2]\n",
    "        text_encoders = [self.text_encoder, self.text_encoder_2] if self.text_encoder is not None else [self.text_encoder_2]\n",
    "\n",
    "        prompt_2 = prompt_2 or prompt\n",
    "        prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2\n",
    "\n",
    "        # textual inversion: procecss multi-vector tokens if necessary\n",
    "        prompt_embeds_list = []\n",
    "        prompts = [prompt, prompt_2]\n",
    "        for prompt, tokenizer, text_encoder in zip(prompts, tokenizers, text_encoders):\n",
    "            text_inputs = tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            text_input_ids = text_inputs.input_ids\n",
    "\n",
    "            prompt_embeds = text_encoder(text_input_ids)\n",
    "\n",
    "            # We are only ALWAYS interested in the pooled output of the final text encoder\n",
    "            pooled_prompt_embeds = prompt_embeds[0]\n",
    "            hidden_states = list(prompt_embeds.values())[1:]\n",
    "            if clip_skip is None:\n",
    "                prompt_embeds = hidden_states[-2]\n",
    "            else:\n",
    "                # \"2\" because SDXL always indexes from the penultimate layer.\n",
    "                prompt_embeds = hidden_states[-(clip_skip + 2)]\n",
    "\n",
    "            prompt_embeds_list.append(prompt_embeds)\n",
    "\n",
    "        prompt_embeds = np.concatenate(prompt_embeds_list, axis=-1)\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        zero_out_negative_prompt = negative_prompt is None\n",
    "        if do_classifier_free_guidance and zero_out_negative_prompt:\n",
    "            negative_prompt_embeds = np.zeros_like(prompt_embeds)\n",
    "            negative_pooled_prompt_embeds = np.zeros_like(pooled_prompt_embeds)\n",
    "        elif do_classifier_free_guidance:\n",
    "            negative_prompt = negative_prompt or \"\"\n",
    "            negative_prompt_2 = negative_prompt_2 or negative_prompt\n",
    "\n",
    "            # normalize str to list\n",
    "            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n",
    "            negative_prompt_2 = batch_size * [negative_prompt_2] if isinstance(negative_prompt_2, str) else negative_prompt_2\n",
    "\n",
    "            uncond_tokens: List[str]\n",
    "            if prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\" f\" {type(prompt)}.\")\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\"\n",
    "                )\n",
    "            else:\n",
    "                uncond_tokens = [negative_prompt, negative_prompt_2]\n",
    "\n",
    "            negative_prompt_embeds_list = []\n",
    "            for negative_prompt, tokenizer, text_encoder in zip(uncond_tokens, tokenizers, text_encoders):\n",
    "                max_length = prompt_embeds.shape[1]\n",
    "                uncond_input = tokenizer(\n",
    "                    negative_prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "                negative_prompt_embeds = text_encoder(uncond_input.input_ids)\n",
    "                # We are only ALWAYS interested in the pooled output of the final text encoder\n",
    "                negative_pooled_prompt_embeds = negative_prompt_embeds[0]\n",
    "                hidden_states = list(negative_prompt_embeds.values())[1:]\n",
    "                negative_prompt_embeds = hidden_states[-2]\n",
    "\n",
    "                negative_prompt_embeds_list.append(negative_prompt_embeds)\n",
    "\n",
    "            negative_prompt_embeds = np.concatenate(negative_prompt_embeds_list, axis=-1)\n",
    "\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "        prompt_embeds = np.tile(prompt_embeds, (1, num_images_per_prompt, 1))\n",
    "        prompt_embeds = prompt_embeds.reshape(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "            seq_len = negative_prompt_embeds.shape[1]\n",
    "            negative_prompt_embeds = np.tile(negative_prompt_embeds, (1, num_images_per_prompt, 1))\n",
    "            negative_prompt_embeds = negative_prompt_embeds.reshape(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        pooled_prompt_embeds = np.tile(pooled_prompt_embeds, (1, num_images_per_prompt)).reshape(bs_embed * num_images_per_prompt, -1)\n",
    "        if do_classifier_free_guidance:\n",
    "            negative_pooled_prompt_embeds = np.tile(negative_pooled_prompt_embeds, (1, num_images_per_prompt)).reshape(bs_embed * num_images_per_prompt, -1)\n",
    "\n",
    "        return (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        )\n",
    "\n",
    "    def prepare_image(\n",
    "        self,\n",
    "        image,\n",
    "        width,\n",
    "        height,\n",
    "        batch_size,\n",
    "        num_images_per_prompt,\n",
    "        do_classifier_free_guidance=False,\n",
    "        guess_mode=False,\n",
    "    ):\n",
    "        image = self.control_image_processor.preprocess(image, height=height, width=width).to(dtype=torch.float32)\n",
    "        image_batch_size = image.shape[0]\n",
    "\n",
    "        if image_batch_size == 1:\n",
    "            repeat_by = batch_size\n",
    "        else:\n",
    "            # image batch size is the same as prompt batch size\n",
    "            repeat_by = num_images_per_prompt\n",
    "\n",
    "        image = image.repeat_interleave(repeat_by, dim=0)\n",
    "\n",
    "        if do_classifier_free_guidance and not guess_mode:\n",
    "            image = torch.cat([image] * 2)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _get_add_time_ids(\n",
    "        self,\n",
    "        original_size,\n",
    "        crops_coords_top_left,\n",
    "        target_size,\n",
    "        text_encoder_projection_dim,\n",
    "    ):\n",
    "        add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "        add_time_ids = torch.tensor([add_time_ids])\n",
    "        return add_time_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compile all parts of OVStableDiffusionXLControlNetPipeline from local openvino model cache or configs. And then initialize OVStableDiffusionXLControlNetPipeline. If model_cache have not be generated, it will cost more time at first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "if NEED_STATIC:\n",
    "    controlnet = core.compile_model(CONTROLNET_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(CONTROLNET_STATIC_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "    unet = core.compile_model(UNET_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(UNET_STATIC_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "    text_encoder = core.compile_model(TEXT_ENCODER_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(TEXT_ENCODER_STATIC_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "    text_encoder_2 = core.compile_model(TEXT_ENCODER_STATIC_2_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(TEXT_ENCODER_STATIC_2_OV_PATH, COMPILE_CONFIG_FP16))\n",
    "    vae_decoder = core.compile_model(VAE_DECODER_STATIC_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(VAE_DECODER_STATIC_OV_PATH, COMPILE_CONFIG_FP32))\n",
    "else:\n",
    "    controlnet = core.compile_model(CONTROLNET_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(COMPILE_CONFIG_FP16))\n",
    "    unet = core.compile_model(UNET_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(COMPILE_CONFIG_FP16))\n",
    "    text_encoder = core.compile_model(TEXT_ENCODER_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(COMPILE_CONFIG_FP16))\n",
    "    text_encoder_2 = core.compile_model(TEXT_ENCODER_2_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(COMPILE_CONFIG_FP16))\n",
    "    vae_decoder = core.compile_model(VAE_DECODER_OV_PATH,device_name=DEVICE_NAME, config=add_cache_dir(COMPILE_CONFIG_FP32))\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(TOKENIZER_OV_PATH)\n",
    "tokenizer_2 = CLIPTokenizer.from_pretrained(TOKENIZER_2_OV_PATH)\n",
    "scheduler = EulerDiscreteScheduler.from_config(SCHEDULER_OV_PATH)\n",
    "\n",
    "\n",
    "ov_pipe = OVStableDiffusionXLControlNetPipeline(\n",
    "    text_encoder=text_encoder,\n",
    "    text_encoder_2=text_encoder_2,\n",
    "    controlnet=controlnet,\n",
    "    unet=unet,\n",
    "    vae_decoder=vae_decoder,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_2=tokenizer_2,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "end_time=time.time()\n",
    "print(\"init pipeline cost time(s): \")\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for numpy and torch to make result reproducible.\n",
    "\n",
    "Set prompt, negative_prompt, image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)           \n",
    "torch.cuda.manual_seed(seed)       \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "negative_prompt = 'low quality, bad quality, sketches'\n",
    "controlnet_conditioning_scale = 0.5\n",
    "\n",
    "image = load_image(\"./hf-logo.png\")\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "image = PIL.Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline infer and save result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "images = ov_pipe(\n",
    "    prompt, negative_prompt=negative_prompt, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale\n",
    "    ).images\n",
    "end_time=time.time()\n",
    "print(\"cost time(s): \")\n",
    "print(end_time-start_time)\n",
    "images[0].save(f\"hug_lab.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will get a image created from hugging-face logo.\n",
    "\n",
    "![alt text](hug_lab.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimum-intel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
