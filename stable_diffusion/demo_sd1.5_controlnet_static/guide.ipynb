{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation and update of environments and dependencies from source. Make sure your python version is greater that 3.10 and your optimum-intel and optimum version is up to date accounding to the requirements.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %python -m venv stable-diffusion-controlnet\n",
    "# %source stable-diffusion-controlnet/bin/activate\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we should convert pytorch model to openvino IR with dynamic shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVStableDiffusionControlNetPipeline\n",
    "import os\n",
    "from diffusers import UniPCMultistepScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set pytroch models of stable diffusion 1.5 and controlnet path if you have them in local, else you can run pipeline from download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD15_PYTORCH_MODEL_DIR=\"stable-diffusion-v1-5\"\n",
    "CONTROLNET_PYTORCH_MODEL_DIR=\"control_v11p_sd15_openpose\"\n",
    "\n",
    "\n",
    "if os.path.exists(SD15_PYTORCH_MODEL_DIR) and os.path.exists(CONTROLNET_PYTORCH_MODEL_DIR):\n",
    "    scheduler = UniPCMultistepScheduler.from_config(\"scheduler_config.json\")\n",
    "    ov_pipe = OVStableDiffusionControlNetPipeline.from_pretrained(SD15_PYTORCH_MODEL_DIR, controlnet_model_id=CONTROLNET_PYTORCH_MODEL_DIR, compile=False, export=True, scheduler=scheduler,device=\"GPU.1\")\n",
    "    ov_pipe.save_pretrained(save_directory=\"./ov_models_dynamic\")\n",
    "    print(\"Dynamic model is saved in ./ov_models_dynamic\")  \n",
    "\n",
    "else:\n",
    "    scheduler = UniPCMultistepScheduler.from_config(\"scheduler_config.json\")\n",
    "    ov_pipe = OVStableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet_model_id=\"lllyasviel/control_v11p_sd15_openpose\", compile=False, export=True, scheduler=scheduler, device=\"GPU.1\")\n",
    "    ov_pipe.save_pretrained(save_directory=\"./ov_models_dynamic\")\n",
    "    print(\"Dynamic model is saved in ./ov_models_dynamic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will have openvino IR models file under **ov_models_dynamic ** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do inference with ov models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVStableDiffusionControlNetPipeline\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "import requests\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommand to use static shape model to decrease GPU memory cost. Set your STATIC_SHAPE and DEVICE_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_STATIC = True\n",
    "STATIC_SHAPE = [1024,1024]\n",
    "DEVICE_NAME = \"GPU.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load openvino model files, if is static, reshape dynamic models to fixed shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NEED_STATIC:\n",
    "    print(\"Using static models\")\n",
    "    scheduler = UniPCMultistepScheduler.from_config(\"scheduler_config.json\")\n",
    "    ov_config ={\"CACHE_DIR\": \"\", 'INFERENCE_PRECISION_HINT': 'f16'}\n",
    "    if not os.path.exists(\"ov_models_static\"):\n",
    "        if os.path.exists(\"ov_models_dynamic\"):\n",
    "            print(\"load dynamic models from local ov files and reshape to static\")\n",
    "            ov_pipe = OVStableDiffusionControlNetPipeline.from_pretrained(Path(\"ov_models_dynamic\"), scheduler=scheduler, device=DEVICE_NAME, compile=True, ov_config=ov_config, height=STATIC_SHAPE[0], width=STATIC_SHAPE[1])\n",
    "            ov_pipe.reshape(batch_size=1 ,height=STATIC_SHAPE[0], width=STATIC_SHAPE[1], num_images_per_prompt=1)\n",
    "            ov_pipe.save_pretrained(save_directory=\"./ov_models_static\")\n",
    "            print(\"Static model is saved in ./ov_models_static\")  \n",
    "        else:\n",
    "            raise ValueError(\"No ov_models_dynamic exists, please trt ov_model_export.py first\")\n",
    "    else:\n",
    "        print(\"load static models from local ov files\")\n",
    "        ov_pipe = OVStableDiffusionControlNetPipeline.from_pretrained(Path(\"ov_models_static\"), scheduler=scheduler, device=DEVICE_NAME, compile=True, ov_config=ov_config, height=STATIC_SHAPE[0], width=STATIC_SHAPE[1])\n",
    "else:\n",
    "    scheduler = UniPCMultistepScheduler.from_config(\"scheduler_config.json\")\n",
    "    ov_config ={\"CACHE_DIR\": \"\", 'INFERENCE_PRECISION_HINT': 'f16'}\n",
    "    print(\"load dynamic models from local ov files\")\n",
    "    ov_pipe = OVStableDiffusionControlNetPipeline.from_pretrained(Path(\"ov_models_dynamic\"), scheduler=scheduler, device=DEVICE_NAME, compile=True, ov_config=ov_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for numpy and torch to make result reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)           \n",
    "torch.cuda.manual_seed(seed)       \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image for controlnet, or you can use your own image, or genereate image with Openpose Openvino model, \n",
    "notice that Openpose model is not supported by OVStableDiffusionControlNetPipeline yet, so you need to convert it to openvino model first manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"pose_1024.png\"):\n",
    "    pose = Image.open(Path(\"pose_1024.png\"))\n",
    "else:\n",
    "    import torch\n",
    "    import openvino as ov\n",
    "    from collections import namedtuple\n",
    "    print(f\"pose.png not found, use openpose to generate pose image.\")\n",
    "    pose_estimator = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "    example_url = \"https://user-images.githubusercontent.com/29454499/224540208-c172c92a-9714-4a7b-857a-b1e54b4d4791.jpg\"\n",
    "    #convert pytorch model of openpose to openvino\n",
    "    class OpenPoseOVModel:\n",
    "        \"\"\"Helper wrapper for OpenPose model inference\"\"\"\n",
    "\n",
    "        def __init__(self, core, model, device=\"AUTO\"):\n",
    "            self.core = core\n",
    "            self.model = model\n",
    "            self.compiled_model = core.compile_model(self.model, device)\n",
    "\n",
    "        def __call__(self, input_tensor: torch.Tensor):\n",
    "            \"\"\"\n",
    "            inference step\n",
    "\n",
    "            Parameters:\n",
    "            input_tensor (torch.Tensor): tensor with prerpcessed input image\n",
    "            Returns:\n",
    "            predicted keypoints heatmaps\n",
    "            \"\"\"\n",
    "            h, w = input_tensor.shape[2:]\n",
    "            input_shape = self.model.input(0).shape\n",
    "            if h != input_shape[2] or w != input_shape[3]:\n",
    "                self.reshape_model(h, w)\n",
    "            results = self.compiled_model(input_tensor)\n",
    "            return torch.from_numpy(results[self.compiled_model.output(0)]), torch.from_numpy(results[self.compiled_model.output(1)])\n",
    "\n",
    "        def reshape_model(self, height: int, width: int):\n",
    "            \"\"\"\n",
    "            helper method for reshaping model to fit input data\n",
    "\n",
    "            Parameters:\n",
    "            height (int): input tensor height\n",
    "            width (int): input tensor width\n",
    "            Returns:\n",
    "            None\n",
    "            \"\"\"\n",
    "            self.model.reshape({0: [1, 3, height, width]})\n",
    "            self.compiled_model = self.core.compile_model(self.model)\n",
    "\n",
    "        def parameters(self):\n",
    "            Device = namedtuple(\"Device\", [\"device\"])\n",
    "            return [Device(torch.device(\"cpu\"))]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            pose_estimator.body_estimation.model,\n",
    "            example_input=torch.zeros([1, 3, 184, 136]),\n",
    "            input=[[1, 3, 184, 136]],\n",
    "        )\n",
    "    print(f\"Converted openpose model to openvino\")\n",
    "    core = ov.Core() \n",
    "    ov_openpose = OpenPoseOVModel(core, ov_model, device=\"CPU\")\n",
    "    pose_estimator.body_estimation.model = ov_openpose  \n",
    "    img = Image.open(requests.get(example_url, stream=True).raw)    \n",
    "    pose = pose_estimator(img)\n",
    "    pose = pose.resize((1024,1024))\n",
    "    pose.save(\"pose_1024.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set prompt, negative_prompt, image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Dancing Darth Vader, best quality, extremely detailed\"\n",
    "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "result = ov_pipe(prompt=prompt, image=pose, num_inference_steps=20, negative_prompt=negative_prompt, height=STATIC_SHAPE[0], width=STATIC_SHAPE[1])\n",
    "\n",
    "result[0].save(\"result_1024.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
